{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":43873,"databundleVersionId":5585780,"sourceType":"competition"}],"dockerImageVersionId":30408,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# End-to-End Image-to-Text Model: Donut (Document Understanding Transformer) üç©\n\nDonut is a combination of a swin encoder and a bart decoder. This means that it can generate text based on an image without needing any OCR. In this notebook, I show how you can use it to train a model for this competition. This is only the starting point, so I'm sure there is much room for improvement. Still, it is so cool that this one model can do everything necessary for this competition!\n\n![donut diagram](https://raw.githubusercontent.com/clovaai/donut/master/misc/overview.png)","metadata":{}},{"cell_type":"code","source":"# You will have to restart the notebook after running this\n!pip install -U datasets transformers pyarrow polyleven -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport re\nimport json\nfrom collections import Counter\nfrom itertools import chain\nfrom pathlib import Path\nfrom typing import List, Dict, Union, Tuple, Any\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import DataLoader\nfrom transformers import (\n    DonutProcessor,\n    VisionEncoderDecoderConfig,\n    VisionEncoderDecoderModel,\n    get_scheduler\n)\nfrom datasets import Dataset\nfrom datasets import Image as ds_img\nfrom polyleven import levenshtein # a faster version of levenshtein","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:46:38.476249Z","iopub.execute_input":"2023-04-04T22:46:38.476899Z","iopub.status.idle":"2023-04-04T22:46:55.685666Z","shell.execute_reply.started":"2023-04-04T22:46:38.476853Z","shell.execute_reply":"2023-04-04T22:46:55.684341Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = Path(\"/kaggle/input/benetech-making-graphs-accessible/train\")\n\nimages_path = data_dir / \"images\"\ntrain_json_files = list((data_dir / \"annotations\").glob(\"*.json\"))\n\n\nclass CFG:\n\n    # General\n    debug = False\n    num_proc = 2\n    num_workers = 2\n    gpus = 2\n\n    # Data\n    max_length = 1024\n    image_height = 560\n    image_width = 560\n\n    # Training\n    epochs = 2\n    val_check_interval = 1.0  # how many times we want to validate during an epoch\n    check_val_every_n_epoch = 1\n    gradient_clip_val = 1.0\n    lr = 3e-5\n    lr_scheduler_type = \"cosine\"\n    num_warmup_steps = 100\n    seed = 42\n    warmup_steps = 300  \n    output_path = \"output\"\n    log_steps = 200\n    batch_size = 2\n    use_wandb = True","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:46:55.693324Z","iopub.execute_input":"2023-04-04T22:46:55.694504Z","iopub.status.idle":"2023-04-04T22:46:56.968337Z","shell.execute_reply.started":"2023-04-04T22:46:55.69446Z","shell.execute_reply":"2023-04-04T22:46:56.967302Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# First step is to transform the given annotations into a format the model can work with\n\nSince the predictions need to be in the form x1;x2;x3 and y1;y2;y3, let's make that the format we will generate. We also need to add special tokens to predict the chart type and separators to distinguish what is what.\n\nRemember, Donut takes an image as input and will generate text as output.","metadata":{}},{"cell_type":"code","source":"# Let's add chart types as special tokens and a special BOS token\n\nPROMPT_TOKEN = \"<|PROMPT|>\"\nX_START = \"<x_start>\"\nX_END = \"<x_end>\"\nY_START = \"<y_start>\"\nY_END = \"<y_end>\"\n\nSEPARATOR_TOKENS = [\n    PROMPT_TOKEN,\n    X_START,\n    X_END,\n    Y_START,\n    Y_END,\n]\n\nLINE_TOKEN =  \"<line>\" \nVERTICAL_BAR_TOKEN = \"<vertical_bar>\"\nHORIZONTAL_BAR_TOKEN = \"<horizontal_bar>\"\nSCATTER_TOKEN = \"<scatter>\"\nDOT_TOKEN = \"<dot>\"\n\nCHART_TYPE_TOKENS = [\n    LINE_TOKEN,\n    VERTICAL_BAR_TOKEN,\n    HORIZONTAL_BAR_TOKEN,\n    SCATTER_TOKEN,\n    DOT_TOKEN,\n]\n\nnew_tokens = SEPARATOR_TOKENS + CHART_TYPE_TOKENS\n\ndef round_float(value: Union[int, float, str]) -> Union[str, float]:\n    \"\"\"\n    Convert a float value to a string with the specified number of decimal places. \n    If there is more than 1 digit in the integer, then we will truncate to 1 decimal.\n    Otherwise, will truncate to 4 decimals.\n\n    Args:\n        value (int, float, str): The float value to convert\n\n    Returns:\n        str: The rounded float value as a string\n    \"\"\"\n    if isinstance(value, float):\n        value = str(value)\n\n        if \".\" in value:\n            integer, decimal = value.split(\".\")\n            if abs(float(integer)) > 1:\n                decimal = decimal[:1]\n            else:\n                decimal = decimal[:4]\n\n            value = integer + \".\" + decimal\n    return value\n\n\ndef is_nan(value: Union[int, float, str]) -> bool:\n    \"\"\"\n    Check if a value is NaN (not a number).\n\n    Args:\n        value (int, float, str): The value to check\n\n    Returns:\n        bool: True if the value is NaN, False otherwise\n    \"\"\"\n    return isinstance(value, float) and str(value) == \"nan\"\n\n\ndef get_gt_string_and_xy(filepath: Union[str, os.PathLike]) -> Dict[str, str]:\n    \"\"\"\n    Get the ground truth string and x-y data from the given JSON file.\n\n    Args:\n        filepath (str): The path to the JSON file\n\n    Returns:\n        dict: A dictionary containing the ground truth string, x-y data, chart type, id, and source\n    \"\"\"\n    filepath = Path(filepath)\n\n    with open(filepath) as fp:\n        data = json.load(fp)\n\n    data_series = data[\"data-series\"]\n\n    all_x, all_y = [], []\n\n    for d in data_series:\n        x = d[\"x\"]\n        y = d[\"y\"]\n\n        x = round_float(x)\n        y = round_float(y)\n\n        # Ignore nan values\n        if is_nan(x) or is_nan(y):\n            continue\n\n        all_x.append(x)\n        all_y.append(y)\n        \n    \n    chart_type = f\"<{data['chart-type']}>\"\n    x_str = X_START + \";\".join(list(map(str, all_x))) + X_END\n    y_str = Y_START + \";\".join(list(map(str, all_y))) + Y_END\n    \n    gt_string = PROMPT_TOKEN + chart_type + x_str + y_str\n\n    return {\n        \"ground_truth\": gt_string,\n        \"x\": json.dumps(all_x),\n        \"y\": json.dumps(all_y),\n        \"chart-type\": data[\"chart-type\"],\n        \"id\": filepath.stem,\n        \"source\": data[\"source\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:46:56.969742Z","iopub.execute_input":"2023-04-04T22:46:56.970117Z","iopub.status.idle":"2023-04-04T22:46:56.985687Z","shell.execute_reply.started":"2023-04-04T22:46:56.970074Z","shell.execute_reply":"2023-04-04T22:46:56.984787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_gt_string_and_xy(data_dir / \"annotations\" / \"000d269c8e26.json\")","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:46:56.99018Z","iopub.execute_input":"2023-04-04T22:46:56.990513Z","iopub.status.idle":"2023-04-04T22:46:57.01336Z","shell.execute_reply.started":"2023-04-04T22:46:56.990485Z","shell.execute_reply":"2023-04-04T22:46:57.012419Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating the dataset","metadata":{}},{"cell_type":"code","source":"# This generator function will be used to create the Dataset object\n\n\ndef gen_data(files: List[Union[str, os.PathLike]]) -> Dict[str, str]:\n    \"\"\"\n    This function takes a list of json files and returns a generator that yields a\n    dictionary with the ground truth string and the path to the image.\n\n    Args:\n        files (list): A list of json files\n\n    Returns:\n        generator: A generator that yields a dictionary with the ground truth string and\n            the path to the corresponding image.\n    \"\"\"\n\n    for f in files:\n\n        yield {\n            **get_gt_string_and_xy(f),\n            \"image_path\": str(images_path / f\"{f.stem}.jpg\"),\n        }\n\n\nds = Dataset.from_generator(\n    gen_data, gen_kwargs={\"files\": train_json_files}, num_proc=CFG.num_proc\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:46:57.014789Z","iopub.execute_input":"2023-04-04T22:46:57.015319Z","iopub.status.idle":"2023-04-04T22:51:11.315366Z","shell.execute_reply.started":"2023-04-04T22:46:57.015282Z","shell.execute_reply":"2023-04-04T22:51:11.314083Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking image sizes","metadata":{}},{"cell_type":"code","source":"def add_image_sizes(examples: Dict[str, Union[str, os.PathLike]]) -> Dict[str, List[int]]:\n    \"\"\"\n    This function takes a dictionary of examples and adds the width and height of the\n    image to the dictionary. This is to be used with the `Dataset.map` function.\n\n    Args:\n        examples (dict): A dictionary of examples (from `map` function)\n\n    Returns:\n        dict: The dictionary with the width and height of the image added\n    \"\"\"\n\n    sizes = [Image.open(x).size for x in examples[\"image_path\"]]\n\n    width, height = list(zip(*sizes))\n\n    return {\n        \"width\": list(width),\n        \"height\": list(height),\n    }\n\n\nds = ds.map(add_image_sizes, batched=True, num_proc=CFG.num_proc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:51:11.317092Z","iopub.execute_input":"2023-04-04T22:51:11.318705Z","iopub.status.idle":"2023-04-04T22:55:14.854321Z","shell.execute_reply.started":"2023-04-04T22:51:11.318635Z","shell.execute_reply":"2023-04-04T22:55:14.853252Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Most fall within 560 x 560\nplt.figure(figsize=(10, 10))\n\nplt.scatter(x=ds[\"width\"], y=ds[\"height\"], marker=\"o\", alpha=0.5)\n\n# Add axis labels\nplt.xlabel(\"Width\")\nplt.ylabel(\"Height\")\n\n# Add a title\nplt.title(\"Image Dimensions\")\n\n# Add gridlines\nplt.grid(True)\n\n# Set aspect ratio to be equal\nplt.gca().set_aspect(\"equal\", adjustable=\"box\")\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:14.856375Z","iopub.execute_input":"2023-04-04T22:55:14.856759Z","iopub.status.idle":"2023-04-04T22:55:15.751319Z","shell.execute_reply.started":"2023-04-04T22:55:14.856718Z","shell.execute_reply":"2023-04-04T22:55:15.7503Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model configuration\n\nNeed to set:  \n\n- image height\n- image width\n- max sequence length to generate","metadata":{}},{"cell_type":"code","source":"config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\nconfig.encoder.image_size = (CFG.image_height, CFG.image_width)\nconfig.decoder.max_length = CFG.max_length\n\nprint(CFG.image_height, CFG.image_width, CFG.max_length)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:15.752832Z","iopub.execute_input":"2023-04-04T22:55:15.75396Z","iopub.status.idle":"2023-04-04T22:55:16.08884Z","shell.execute_reply.started":"2023-04-04T22:55:15.753918Z","shell.execute_reply":"2023-04-04T22:55:16.08778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Checking tokenizer for unknowns\n","metadata":{}},{"cell_type":"code","source":"processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\nprocessor.image_processor.size = {\n    \"height\": CFG.image_height,\n    \"width\": CFG.image_width,\n}","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:16.090542Z","iopub.execute_input":"2023-04-04T22:55:16.091247Z","iopub.status.idle":"2023-04-04T22:55:18.921236Z","shell.execute_reply.started":"2023-04-04T22:55:16.091207Z","shell.execute_reply":"2023-04-04T22:55:18.920175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_for_unk(examples: Dict[str, str]) -> Dict[str, List[str]]:\n    \"\"\"\n    Check for unknown tokens in the given examples.\n\n    This function takes a dictionary containing a list of ground truth texts and\n    tokenizes them using the processor's tokenizer. It then checks for any unknown\n    tokens in the tokenized text and returns a dictionary containing a list of the\n    unknown tokens for each example.\n\n    Args:\n        examples (dict): A dictionary containing a list of ground truth texts. \n            Example: {\"ground_truth\": [\"text1\", \"text2\", ...]}\n\n    Returns:\n        dict: A dictionary containing a list of unknown tokens for each example. \n            Example: {\"unk_tokens\": [[\"unk1\", \"unk2\"], [], [\"unk3\"], ...]}\n    \"\"\"\n\n    texts = examples[\"ground_truth\"]\n\n    ids = processor.tokenizer(texts).input_ids\n    tokens = [processor.tokenizer.tokenize(x, add_special_tokens=True) for x in texts]\n\n    unk_tokens = []\n    for example_ids, example_tokens in zip(ids, tokens):\n        example_unk_tokens = []\n        for i in range(len(example_ids)):\n            if example_ids[i] == processor.tokenizer.unk_token_id:\n                example_unk_tokens.append(example_tokens[i])\n\n        unk_tokens.append(example_unk_tokens)\n\n    return {\"unk_tokens\": unk_tokens}\n\n\nunk = ds.map(check_for_unk, batched=True, num_proc=CFG.num_proc)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:18.922536Z","iopub.execute_input":"2023-04-04T22:55:18.922911Z","iopub.status.idle":"2023-04-04T22:55:48.094523Z","shell.execute_reply.started":"2023-04-04T22:55:18.922874Z","shell.execute_reply":"2023-04-04T22:55:48.093411Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's look at only the examples with unknown tokens\nunk = unk.filter(lambda x: len(x[\"unk_tokens\"]) > 0, num_proc=CFG.num_proc)\n\nprint(len(unk))\n\nunk[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:48.09627Z","iopub.execute_input":"2023-04-04T22:55:48.096987Z","iopub.status.idle":"2023-04-04T22:55:49.379855Z","shell.execute_reply.started":"2023-04-04T22:55:48.096953Z","shell.execute_reply":"2023-04-04T22:55:49.378563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's count which tokens show up most often as unknowns\nall_unk_tokens = [x for y in unk[\"unk_tokens\"] for x in y]\n\nCounter(all_unk_tokens)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:49.381907Z","iopub.execute_input":"2023-04-04T22:55:49.382596Z","iopub.status.idle":"2023-04-04T22:55:49.692224Z","shell.execute_reply.started":"2023-04-04T22:55:49.382554Z","shell.execute_reply":"2023-04-04T22:55:49.691044Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that the donut tokenizer doesn't contain \"1\" which will result in many unknown tokens in the ground truth string. There is \"_1\" (space in front of 1) and \"01\", \"11\", \"21\", \"31\", \"41\" etc. But if there is a \"0.1\" or \">1\" then the 1 will turn into an unknown token.\n\nI'll handle this in the `preprocess` function.","metadata":{}},{"cell_type":"code","source":"example_str = \"0.1 1 1990\"\n\ntemp_ids = processor.tokenizer(example_str).input_ids\nprint(\"ids:\", temp_ids)\nprint(\"tokenized:\", processor.tokenizer.tokenize(example_str))\nprint(\"decoded:\", processor.tokenizer.decode(temp_ids))\nprint(\"unk id:\", processor.tokenizer.unk_token_id)\n\n# Adding these tokens should mean that there should be very few unknown tokens\nnum_added = processor.tokenizer.add_tokens([\"<one>\"] + new_tokens)\nprint(num_added, \"tokens added\")\n\nconfig.pad_token_id = processor.tokenizer.pad_token_id\nconfig.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([PROMPT_TOKEN])[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:49.697541Z","iopub.execute_input":"2023-04-04T22:55:49.697842Z","iopub.status.idle":"2023-04-04T22:55:49.707358Z","shell.execute_reply.started":"2023-04-04T22:55:49.697814Z","shell.execute_reply":"2023-04-04T22:55:49.706113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing function to get pixel values and input_ids\n\nWhen ever the `_getitem__` function is called for the dataset, it will run this function.","metadata":{}},{"cell_type":"code","source":"from functools import partial\n\none_token_id = processor.tokenizer(\"<one>\", add_special_tokens=False).input_ids[0]\nunk_token_id = processor.tokenizer.unk_token_id\n\n\ndef replace_unk_tokens_with_one(example_ids: List[int], example_tokens: List[str], one_token_id:int, unk_token_id:int) -> List[int]:\n    \"\"\"\n    Replace unknown tokens that represent \"1\" with the correct token id.\n\n    Args:\n        example_ids (list): List of token ids for a given example\n        example_tokens (list): List of tokens for the same given example\n        one_token_id (int): Token id for the \"<one>\" token\n        unk_token_id (int): Token id for the unknown token\n\n    Returns:\n        list: The updated list of token ids with the correct token id for \"1\"\n    \"\"\"\n    \n    temp_ids = []\n    for id_, token in zip(example_ids, example_tokens):\n        if id_ == unk_token_id and token == \"1\":\n            id_ = one_token_id\n        temp_ids.append(id_)\n    return temp_ids\n\n\ndef preprocess(examples: Dict[str, str], processor: DonutProcessor, CFG: CFG) -> Dict[str, Union[torch.Tensor, List[int], List[str]]]:\n    \"\"\"\n    Preprocess the given examples.\n\n    This function processes the input examples by tokenizing the texts, replacing\n    any unknown tokens that represent \"1\" with the correct token id, and loading\n    the images.\n\n    Args:\n        examples (dict): A dictionary containing ground truth texts, image paths, and ids\n        processor: An object responsible for tokenizing texts and processing images\n        CFG: A configuration object containing settings and hyperparameters\n\n    Returns:\n        dict: A dictionary containing preprocessed images, token ids, and ids\n    \"\"\"\n    \n    pixel_values = []\n\n    texts = examples[\"ground_truth\"]\n\n    ids = processor.tokenizer(\n        texts,\n        add_special_tokens=False,\n        max_length=CFG.max_length,\n        padding=True,\n        truncation=True,\n    ).input_ids\n\n    if isinstance(texts, str):\n        texts = [texts]\n\n    tokens = [processor.tokenizer.tokenize(text, add_special_tokens=False) for text in texts]\n    \n    one_token_id = processor.tokenizer(\"<one>\", add_special_tokens=False).input_ids[0]\n    unk_token_id = processor.tokenizer.unk_token_id\n    \n    final_ids = [\n        replace_unk_tokens_with_one(example_ids, example_tokens, one_token_id, unk_token_id)\n        for example_ids, example_tokens in zip(ids, tokens)\n    ]\n\n    for sample in examples[\"image_path\"]:\n        pixel_values.append(processor(sample, random_padding=True).pixel_values)\n\n    return {\n        \"pixel_values\": torch.tensor(np.vstack(pixel_values)),\n        \"input_ids\": final_ids,\n        \"id\": examples[\"id\"],\n    }\n\n\nimage_ds = ds.cast_column(\"image_path\", ds_img())\nimage_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:49.709219Z","iopub.execute_input":"2023-04-04T22:55:49.709656Z","iopub.status.idle":"2023-04-04T22:55:49.77207Z","shell.execute_reply.started":"2023-04-04T22:55:49.709618Z","shell.execute_reply":"2023-04-04T22:55:49.77113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = image_ds[[0, 1, 2]]\n\nprint(sample[\"pixel_values\"].shape)\nprint(processor.decode(sample[\"input_ids\"][2]))\nprint(len(sample[\"input_ids\"][2]))\nprint(processor.tokenizer.convert_ids_to_tokens(sample[\"input_ids\"][2]))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:49.773462Z","iopub.execute_input":"2023-04-04T22:55:49.773788Z","iopub.status.idle":"2023-04-04T22:55:49.996673Z","shell.execute_reply.started":"2023-04-04T22:55:49.773753Z","shell.execute_reply":"2023-04-04T22:55:49.995112Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating splits\n\n\nBasic split: break extracted into folds, all generated and some extracted into train. Only extracted in validation.","metadata":{}},{"cell_type":"code","source":"# separate by extracted and generated\n\nextracted_ds = ds.filter(lambda x: x[\"source\"] == \"extracted\", num_proc=CFG.num_proc)\ngenerated_ds = ds.filter(lambda x: x[\"source\"] == \"generated\", num_proc=CFG.num_proc)\n\nchart_types = extracted_ds[\"chart-type\"]\n\nprint(Counter(chart_types))\n\nskf = StratifiedKFold(n_splits=4)\n\nfold_idxs = []\n\nfor _, val_idxs in skf.split(chart_types, y=chart_types):\n    fold_idxs.append(val_idxs)\n\n\nfor n, idxs in enumerate(fold_idxs):\n    print(Counter([chart_types[i] for i in idxs]))\n    if n > 3:\n        break","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:49.99813Z","iopub.execute_input":"2023-04-04T22:55:49.998821Z","iopub.status.idle":"2023-04-04T22:55:52.154257Z","shell.execute_reply.started":"2023-04-04T22:55:49.998774Z","shell.execute_reply":"2023-04-04T22:55:52.152932Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine generated and some of the extracted examples into the train set\n\nfrom datasets import concatenate_datasets\n\nfold = 0\n\ntrain_extracted = extracted_ds.select(\n    list(chain(*[x for i, x in enumerate(fold_idxs) if i != fold]))\n)\ntrain_ds = concatenate_datasets([train_extracted, generated_ds])\ntrain_ds = train_ds.cast_column(\"image_path\", ds_img())\ntrain_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))\n\n# Creat validation set from only extracted examples\n\nval_gt_ds = extracted_ds.select(fold_idxs[fold])\nval_ds = val_gt_ds.cast_column(\"image_path\", ds_img())\nval_ds.set_transform(partial(preprocess, processor=processor, CFG=CFG))\n\ngt_chart_type = val_gt_ds[\"chart-type\"]\ngt_x = [json.loads(_) for _ in val_gt_ds[\"x\"]]\ngt_y = [json.loads(_) for _ in val_gt_ds[\"y\"]]\ngt_ids = val_gt_ds[\"id\"]\n\ni = 0\nprint(gt_chart_type[i])\nprint(gt_x[i])\nprint(gt_y[i])\nprint(gt_ids[i])\nprint(Counter(gt_chart_type))","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:52.156038Z","iopub.execute_input":"2023-04-04T22:55:52.156323Z","iopub.status.idle":"2023-04-04T22:55:52.371442Z","shell.execute_reply.started":"2023-04-04T22:55:52.156294Z","shell.execute_reply":"2023-04-04T22:55:52.370217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Collate function to make sure the ids are all the same length in a batch","metadata":{}},{"cell_type":"code","source":"pad_token_id = processor.tokenizer.pad_token_id\n\n\ndef collate_fn(samples: List[Dict[str, Union[torch.Tensor, List[int], str]]]) -> Dict[str, Union[torch.Tensor, List[str]]]:\n    \"\"\"\n    Custom collate function for DataLoader.\n\n    This function takes a list of samples and combines them into a batch with\n    properly padded input_ids.\n\n    Args:\n        samples (List[Dict[str, Union[torch.Tensor, List[int], str]]]): \n            A list of samples, where each sample is a dictionary containing\n            \"pixel_values\" (torch.Tensor), \"input_ids\" (List[int]), and \"id\" (str).\n\n    Returns:\n        Dict[str, Union[torch.Tensor, List[str]]]: \n            A dictionary containing the combined pixel values, padded input_ids, and ids.\n    \"\"\"\n\n    batch = {}\n\n    batch[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in samples])\n\n    max_length = max([len(x[\"input_ids\"]) for x in samples])\n\n    # Make a multiple of 8 to efficiently use the tensor cores\n    if max_length % 8 != 0:\n        max_length = (max_length // 8 + 1) * 8\n\n    input_ids = [\n        x[\"input_ids\"] + [pad_token_id] * (max_length - len(x[\"input_ids\"]))\n        for x in samples\n    ]\n\n    labels = torch.tensor(input_ids)\n    labels[labels == pad_token_id] = -100 # ignore loss on padding tokens\n    batch[\"labels\"] = labels\n    \n    batch[\"id\"] = [x[\"id\"] for x in samples]\n\n    return batch\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:52.373137Z","iopub.execute_input":"2023-04-04T22:55:52.37353Z","iopub.status.idle":"2023-04-04T22:55:52.382791Z","shell.execute_reply.started":"2023-04-04T22:55:52.373492Z","shell.execute_reply":"2023-04-04T22:55:52.381613Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataloaders\n\nValidation uses generation so it is very slow. That's why I only use a small fraction of the examples.","metadata":{}},{"cell_type":"code","source":"if CFG.debug:\n    train_ds = train_ds.select(range(100))\n    val_ds = val_ds.select(range(100))\n\ntrain_dataloader = DataLoader(\n    train_ds,\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=CFG.num_workers,\n)\nval_dataloader = DataLoader(\n    val_ds,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=CFG.num_workers,\n)\n\nnum_training_steps = len(train_dataloader) * CFG.epochs // CFG.gpus\n\nbatch = next(iter(train_dataloader))\n\nbatch.keys(), [(k, v.shape) for k, v in batch.items() if k != \"id\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:52.384731Z","iopub.execute_input":"2023-04-04T22:55:52.385571Z","iopub.status.idle":"2023-04-04T22:55:52.893685Z","shell.execute_reply.started":"2023-04-04T22:55:52.385525Z","shell.execute_reply":"2023-04-04T22:55:52.892533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions to calculate metrics","metadata":{}},{"cell_type":"code","source":"def rmse(y_true: List[float], y_pred: List[float]) -> float:\n    \"\"\"\n    Calculate the Root Mean Square Error (RMSE) between the true and predicted values.\n\n    Args:\n        y_true (List[float]): The true values.\n        y_pred (List[float]): The predicted values.\n\n    Returns:\n        float: The Root Mean Square Error.\n    \"\"\"\n    return np.sqrt(np.mean(np.square(np.subtract(y_true, y_pred))))\n\n\ndef sigmoid(x: float) -> float:\n    \"\"\"\n    Calculate the sigmoid function for the given value.\n\n    Args:\n        x (float): The input value.\n\n    Returns:\n        float: The result of the sigmoid function.\n    \"\"\"\n    return 2 - 2 / (1 + np.exp(-x))\n\n\ndef normalized_rmse(y_true: List[float], y_pred: List[float]) -> float:\n    \"\"\"\n    Calculate the normalized Root Mean Square Error (RMSE) between the true and predicted values.\n\n    Args:\n        y_true (List[float]): The true values.\n        y_pred (List[float]): The predicted values.\n\n    Returns:\n        float: The normalized Root Mean Square Error.\n    \"\"\"\n    numerator = rmse(y_true, y_pred)\n    denominator = rmse(y_true, np.mean(y_true))\n\n    # https://www.kaggle.com/competitions/benetech-making-graphs-accessible/discussion/396947\n    if denominator == 0:\n        if numerator == 0:\n            return 1.0\n        return 0.0\n\n    return sigmoid(numerator / denominator)\n\n\ndef normalized_levenshtein_score(y_true: List[str], y_pred: List[str]) -> float:\n    \"\"\"\n    Calculate the normalized Levenshtein distance between two lists of strings.\n\n    Args:\n        y_true (List[str]): The true values.\n        y_pred (List[str]): The predicted values.\n\n    Returns:\n        float: The normalized Levenshtein distance.\n    \"\"\"\n    total_distance = np.sum([levenshtein(yt, yp) for yt, yp in zip(y_true, y_pred)])\n    length_sum = np.sum([len(yt) for yt in y_true])\n    return sigmoid(total_distance / length_sum)\n\n\ndef score_series(\n    y_true: List[Union[float, str]], y_pred: List[Union[float, str]]\n) -> float:\n    \"\"\"\n    Calculate the score for a series of true and predicted values.\n\n    Args:\n        y_true (List[Union[float, str]]): The true values.\n        y_pred (List[Union[float, str]]): The predicted values.\n\n    Returns:\n        float: The score for the series.\n    \"\"\"\n    if len(y_true) != len(y_pred):\n        return 0.0\n    if isinstance(y_true[0], str):\n        return normalized_levenshtein_score(y_true, y_pred)\n    else:\n        # Since this is a generative model, there is a chance it doesn't produce a float.\n        # In that case, we return 0.0.\n        try:\n            return normalized_rmse(y_true, list(map(float, y_pred)))\n        except:\n            return 0.0\n\n\ndef benetech_score(ground_truth: pd.DataFrame, predictions: pd.DataFrame) -> float:\n    \"\"\"Evaluate predictions using the metric from the Benetech - Making Graphs Accessible.\n\n    Parameters\n    ----------\n    ground_truth: pd.DataFrame\n        Has columns `[data_series, chart_type]` and an index `id`. Values in `data_series`\n        should be either arrays of floats or arrays of strings.\n\n    predictions: pd.DataFrame\n    \"\"\"\n    if not ground_truth.index.equals(predictions.index):\n        raise ValueError(\n            \"Must have exactly one prediction for each ground-truth instance.\"\n        )\n    if not ground_truth.columns.equals(predictions.columns):\n        raise ValueError(f\"Predictions must have columns: {ground_truth.columns}.\")\n    pairs = zip(\n        ground_truth.itertuples(index=False), predictions.itertuples(index=False)\n    )\n    scores = []\n    for (gt_series, gt_type), (pred_series, pred_type) in pairs:\n        if gt_type != pred_type:  # Check chart_type condition\n            scores.append(0.0)\n        else:  # Score with RMSE or Levenshtein as appropriate\n            scores.append(score_series(gt_series, pred_series))\n\n    ground_truth[\"score\"] = scores\n\n    grouped = ground_truth.groupby(\"chart_type\", as_index=False)[\"score\"].mean()\n\n    chart_type2score = {\n        chart_type: score\n        for chart_type, score in zip(grouped[\"chart_type\"], grouped[\"score\"])\n    }\n\n    return np.mean(scores), chart_type2score\n\n\ndef string2triplet(pred_string: str) -> Tuple[str, List[str], List[str]]:\n    \"\"\"\n    Convert a prediction string to a triplet of chart type, x values, and y values.\n\n    Args:\n        pred_string (str): The prediction string.\n\n    Returns:\n        Tuple[str, List[str], List[str]]: A triplet of chart type, x values, and y values.\n    \"\"\"\n    \n    chart_type = \"line\"\n    for tok in CHART_TYPE_TOKENS:\n        if tok in pred_string:\n            chart_type = tok.strip(\"<>\")\n\n    pred_string = re.sub(r\"<one>\", \"1\", pred_string)\n\n    x = pred_string.split(X_START)[1].split(X_END)[0].split(\";\")\n    y = pred_string.split(Y_START)[1].split(Y_END)[0].split(\";\")\n\n    if len(x) == 0 or len(y) == 0:\n        return chart_type, [], []\n\n    min_length = min(len(x), len(y))\n\n    x = x[:min_length]\n    y = y[:min_length]\n\n    return chart_type, x, y\n\n\ndef validation_metrics(val_outputs: List[str], val_ids: List[str], gt_df: pd.DataFrame) -> Dict[str, float]:\n    \"\"\"\n    Calculate validation metrics for a set of outputs, ids, and ground truth dataframe.\n\n    Args:\n        val_outputs (List[str]): A list of validation outputs.\n        val_ids (List[str]): A list of validation ids.\n        gt_df (pd.DataFrame): The ground truth dataframe.\n\n    Returns:\n        Dict[str, float]: A dictionary containing the validation scores.\n    \"\"\"\n    pred_triplets = []\n\n    for example_output in val_outputs:\n\n        if not all([x in example_output for x in [X_START, X_END, Y_START, Y_END]]):\n            pred_triplets.append((\"line\", [], []))\n        else:\n            pred_triplets.append(string2triplet(example_output))\n\n    pred_df = pd.DataFrame(\n        index=[f\"{id_}_x\" for id_ in val_ids] + [f\"{id_}_y\" for id_ in val_ids],\n        data={\n            \"data_series\": [x[1] for x in pred_triplets]\n            + [x[2] for x in pred_triplets],\n            \"chart_type\": [x[0] for x in pred_triplets] * 2,\n        },\n    )\n\n    overall_score, chart_type2score = benetech_score(\n        gt_df.loc[pred_df.index.values], pred_df\n    )\n\n    return {\n        \"val_score\": overall_score,\n        **{f\"{k}_score\": v for k, v in chart_type2score.items()},\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:52.89568Z","iopub.execute_input":"2023-04-04T22:55:52.896307Z","iopub.status.idle":"2023-04-04T22:55:52.921083Z","shell.execute_reply.started":"2023-04-04T22:55:52.896265Z","shell.execute_reply":"2023-04-04T22:55:52.920105Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lightning Module\n\nIt will use generation for validation which can be very slow. If you want to utilize the two GPUs on Kaggle, you should probably not run it in a notebook because DDP gets funky when doing that.","metadata":{}},{"cell_type":"code","source":"import pytorch_lightning as pl\nfrom transformers import PreTrainedTokenizerBase, PreTrainedModel\n\nclass DonutModelPLModule(pl.LightningModule):\n    def __init__(self, processor: PreTrainedTokenizerBase, model: PreTrainedModel, gt_df: pd.DataFrame, num_training_steps: int):\n        \"\"\"\n        A PyTorch Lightning module for the DonutModel.\n\n        Args:\n            processor (PreTrainedTokenizerBase): The tokenizer/processor for the model.\n            model (PreTrainedModel): The pretrained model.\n            gt_df (pd.DataFrame): The ground truth dataframe.\n            num_training_steps (int): The number of training steps.\n        \"\"\"\n        super().__init__()\n        self.processor = processor\n        self.model = model\n        self.gt_df = gt_df\n        self.num_training_steps = num_training_steps\n\n    def training_step(self, batch: Dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n        pixel_values = batch[\"pixel_values\"]\n        labels = batch[\"labels\"]\n\n        outputs = self.model(pixel_values, labels=labels)\n        loss = outputs.loss\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int, dataset_idx: int = 0) -> None:\n        pixel_values = batch[\"pixel_values\"]\n        batch_size = pixel_values.shape[0]\n        # we feed the prompt to the model\n        decoder_input_ids = torch.full(\n            (batch_size, 1),\n            self.model.config.decoder_start_token_id,\n            device=self.device,\n        )\n\n        outputs = self.model.generate(\n            pixel_values,\n            decoder_input_ids=decoder_input_ids,\n            max_length=CFG.max_length,\n            early_stopping=True,\n            pad_token_id=self.processor.tokenizer.pad_token_id,\n            eos_token_id=self.processor.tokenizer.eos_token_id,\n            use_cache=True,\n            num_beams=1,\n            top_k=1,\n            bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n            return_dict_in_generate=True,\n        )\n\n        self.val_outputs.extend(\n            self.processor.tokenizer.batch_decode(outputs.sequences)\n        )\n        self.val_ids.extend(batch[\"id\"])\n\n    def on_validation_start(self) -> None:\n        self.val_outputs, self.val_ids = [], []\n\n    def validation_epoch_end(self, outputs: List[Any]) -> None:\n\n        metrics = validation_metrics(self.val_outputs, self.val_ids, self.gt_df)\n        print(\"\\n\", metrics)\n\n        self.log_dict(metrics)\n\n        self.val_outputs, self.val_ids = [], []\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        \n        optimizer = torch.optim.Adam(self.parameters(), lr=CFG.lr)\n\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:52.92272Z","iopub.execute_input":"2023-04-04T22:55:52.923125Z","iopub.status.idle":"2023-04-04T22:55:54.855523Z","shell.execute_reply.started":"2023-04-04T22:55:52.923088Z","shell.execute_reply":"2023-04-04T22:55:54.854429Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create model and ground truth dataframe","metadata":{}},{"cell_type":"code","source":"gt_chart_type = val_gt_ds[\"chart-type\"]\ngt_x = [json.loads(_) for _ in val_gt_ds[\"x\"]]\ngt_y = [json.loads(_) for _ in val_gt_ds[\"y\"]]\ngt_ids = val_gt_ds[\"id\"]\n\nindex = [f\"{id_}_x\" for id_ in gt_ids] + [f\"{id_}_y\" for id_ in gt_ids]\ngt_df = pd.DataFrame(\n    index=index,\n    data={\n        \"data_series\": gt_x + gt_y,\n        \"chart_type\": gt_chart_type * 2,\n    },\n)\n\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\n    \"naver-clova-ix/donut-base\", config=config\n)\nmodel.decoder.resize_token_embeddings(len(processor.tokenizer))\nmodel_module = DonutModelPLModule(processor, model, gt_df, num_training_steps)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:55:54.857065Z","iopub.execute_input":"2023-04-04T22:55:54.857465Z","iopub.status.idle":"2023-04-04T22:56:09.792729Z","shell.execute_reply.started":"2023-04-04T22:55:54.857378Z","shell.execute_reply":"2023-04-04T22:56:09.791446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train!","metadata":{}},{"cell_type":"code","source":"from pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.loggers import WandbLogger\n\ncheckpoint_cb = ModelCheckpoint(CFG.output_path)\n\nloggers = []\nif CFG.use_wandb:\n    import wandb\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    key = user_secrets.get_secret(\"wandb\")\n    wandb.login(key=key)\n    \n    loggers.append(WandbLogger(project=\"benetech\"))\n\n\ntrainer = pl.Trainer(\n        accelerator=\"gpu\",\n        devices=1,\n        max_epochs=CFG.epochs,\n        val_check_interval=CFG.val_check_interval,\n        check_val_every_n_epoch=CFG.check_val_every_n_epoch,\n        gradient_clip_val=CFG.gradient_clip_val,\n        precision=16, # if you have tensor cores (t4, v100, a100, etc.) training will be 2x faster\n        num_sanity_val_steps=5,\n        callbacks=[checkpoint_cb], \n        logger=loggers\n)\n\ntrainer.fit(model_module, train_dataloaders=train_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T22:56:09.79413Z","iopub.execute_input":"2023-04-04T22:56:09.794834Z","iopub.status.idle":"2023-04-05T04:39:30.160099Z","shell.execute_reply.started":"2023-04-04T22:56:09.794791Z","shell.execute_reply":"2023-04-05T04:39:30.158486Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I interrupted the run because I was impatient","metadata":{}},{"cell_type":"code","source":"trainer.validate(model_module, dataloaders=val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:39:56.863039Z","iopub.execute_input":"2023-04-05T04:39:56.863632Z","iopub.status.idle":"2023-04-05T04:54:42.779124Z","shell.execute_reply.started":"2023-04-05T04:39:56.863593Z","shell.execute_reply":"2023-04-05T04:54:42.77779Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Could also use the checkpoint callback\nmodel_module.model.save_pretrained(CFG.output_path)\nmodel_module.processor.save_pretrained(CFG.output_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T04:54:58.97871Z","iopub.execute_input":"2023-04-05T04:54:58.979678Z","iopub.status.idle":"2023-04-05T04:55:00.733355Z","shell.execute_reply.started":"2023-04-05T04:54:58.979635Z","shell.execute_reply":"2023-04-05T04:55:00.732081Z"},"trusted":true},"outputs":[],"execution_count":null}]}